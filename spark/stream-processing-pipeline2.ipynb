{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, Row, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    BooleanType,\n",
    "    FloatType,\n",
    ")\n",
    "from time import sleep\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"StreamReviews\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "sparkConf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# We need to set the following configuration whenever we need to use GCS.\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\n",
    "    \"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\"\n",
    ")\n",
    "\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"temp_de2023_20204025\"\n",
    "spark.conf.set(\"temporaryGcsBucket\", bucket)\n",
    "\n",
    "\n",
    "dataSchema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"reviewId\", StringType(), True),\n",
    "        StructField(\"creationDate\", StringType(), True),\n",
    "        StructField(\"criticName\", StringType(), True),\n",
    "        StructField(\"isTopCritic\", BooleanType(), True),\n",
    "        StructField(\"originalScore\", StringType(), True),\n",
    "        StructField(\"reviewState\", StringType(), True),\n",
    "        StructField(\"publicatioName\", StringType(), True),\n",
    "        StructField(\"reviewText\", StringType(), True),\n",
    "        StructField(\"scoreSentiment\", StringType(), True),\n",
    "        StructField(\"reviewUrl\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read the whole dataset as a batch\n",
    "kafkaStream = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\")\n",
    "    .option(\"subscribe\", \"review\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df = kafkaStream.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df1 = df.select(from_json(df.value, dataSchema.simpleString()))\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "sdf = df1.select(col(\"from_json(value).*\"))\n",
    "\n",
    "# Converting releaseDateTheaters to datetime\n",
    "review_df = sdf.withColumn(\n",
    "    \"creationDate\", to_date(col(\"creationDate\"), \"y-M-d\")\n",
    ")  # our dataframe with date column\n",
    "\n",
    "# Define the grade dictionary as a broadcast variable for efficient use in UDF\n",
    "grade_dct = spark.sparkContext.broadcast(\n",
    "    {\n",
    "        \"A+\": 0.985,\n",
    "        \"A\": 0.945,\n",
    "        \"A-\": 0.91,\n",
    "        \"B+\": 0.88,\n",
    "        \"B\": 0.845,\n",
    "        \"B-\": 0.81,\n",
    "        \"C+\": 0.78,\n",
    "        \"C\": 0.745,\n",
    "        \"C-\": 0.71,\n",
    "        \"D+\": 0.68,\n",
    "        \"D\": 0.645,\n",
    "        \"D-\": 0.61,\n",
    "        \"F\": 0.295,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Define the UDF for fixing the rating\n",
    "def fix_rating_udf(x):\n",
    "    try:\n",
    "        if x in grade_dct.value.keys():\n",
    "            return grade_dct.value[x]\n",
    "        else:\n",
    "            split_values = list(\n",
    "                map(\n",
    "                    lambda s: float(\n",
    "                        s.replace('\"', \"\").replace(\"'\", \"\").replace(\"*\", \"\").strip()\n",
    "                    ),\n",
    "                    x.split(\"/\"),\n",
    "                )\n",
    "            )\n",
    "            return split_values[0] / split_values[1]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Register the UDF\n",
    "fix_rating_spark_udf = udf(fix_rating_udf, FloatType())\n",
    "\n",
    "# Apply the UDF to the 'originalScore' column\n",
    "review_df = review_df.withColumn(\n",
    "    \"originalScore\", fix_rating_spark_udf(col(\"originalScore\"))\n",
    ")\n",
    "\n",
    "# Apply additional conditions to 'originalScore' column\n",
    "review_df = review_df.withColumn(\n",
    "    \"originalScore\", when(col(\"originalScore\") > 1, 1).otherwise(col(\"originalScore\"))\n",
    ")\n",
    "review_df = review_df.withColumn(\n",
    "    \"originalScore\", when(col(\"originalScore\") < 0, 0).otherwise(col(\"originalScore\"))\n",
    ")\n",
    "\n",
    "\n",
    "avgratingf = review_df.groupBy(window(col(\"creationDate\"), \"1 day\")).agg(\n",
    "    avg(\"originalScore\").alias(\"avg_score_day\")\n",
    ")\n",
    "\n",
    "\n",
    "def my_foreach_batch_function_2(df, batch_id):\n",
    "    # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format(\"bigquery\").option(\n",
    "        \"table\", \"de-23-lab-1-399021.assignment2dataset.avgrating\"\n",
    "    ).mode(\"overwrite\").save()\n",
    "\n",
    "\n",
    "avg_ratingQuery = (\n",
    "    avgratingf.writeStream.outputMode(\"complete\")\n",
    "    .trigger(processingTime=\"2 seconds\")\n",
    "    .foreachBatch(my_foreach_batch_function_2)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "try:\n",
    "    avg_ratingQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    avg_ratingQuery.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
