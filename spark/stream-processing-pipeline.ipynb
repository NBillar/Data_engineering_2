{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\20202016\\Documents\\Master\\Data Engineering\\assignments\\Data_engineering_2\\spark\\stream-processing-pipeline.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202016/Documents/Master/Data%20Engineering/assignments/Data_engineering_2/spark/stream-processing-pipeline.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m sparkConf\u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mspark.driver.cores\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202016/Documents/Master/Data%20Engineering/assignments/Data_engineering_2/spark/stream-processing-pipeline.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# create the spark session, which is the entry point to Spark SQL engine.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/20202016/Documents/Master/Data%20Engineering/assignments/Data_engineering_2/spark/stream-processing-pipeline.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mconfig(conf\u001b[39m=\u001b[39;49msparkConf)\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202016/Documents/Master/Data%20Engineering/assignments/Data_engineering_2/spark/stream-processing-pipeline.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m dataSchema \u001b[39m=\u001b[39m StructType(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202016/Documents/Master/Data%20Engineering/assignments/Data_engineering_2/spark/stream-processing-pipeline.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     [\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202016/Documents/Master/Data%20Engineering/assignments/Data_engineering_2/spark/stream-processing-pipeline.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m, StringType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202016/Documents/Master/Data%20Engineering/assignments/Data_engineering_2/spark/stream-processing-pipeline.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202016/Documents/Master/Data%20Engineering/assignments/Data_engineering_2/spark/stream-processing-pipeline.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20202016/Documents/Master/Data%20Engineering/assignments/Data_engineering_2/spark/stream-processing-pipeline.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Read the whole dataset as a batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20202016\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\20202016\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\20202016\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\20202016\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\20202016\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\java_gateway.py:104\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 104\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mJAVA_GATEWAY_EXITED\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[39m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, Row, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    BooleanType,\n",
    "    FloatType,\n",
    ")\n",
    "from time import sleep\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"StreamReviews\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# We need to set the following configuration whenever we need to use GCS.\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\n",
    "    \"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\"\n",
    ")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"temp_de2023_2065718\"\n",
    "spark.conf.set(\"temporaryGcsBucket\", bucket)\n",
    "\n",
    "\n",
    "dataSchema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"reviewId\", StringType(), True),\n",
    "        StructField(\"creationDate\", StringType(), True),\n",
    "        StructField(\"criticName\", StringType(), True),\n",
    "        StructField(\"isTopCritic\", BooleanType(), True),\n",
    "        StructField(\"originalScore\", StringType(), True),\n",
    "        StructField(\"reviewState\", StringType(), True),\n",
    "        StructField(\"publicatioName\", StringType(), True),\n",
    "        StructField(\"reviewText\", StringType(), True),\n",
    "        StructField(\"scoreSentiment\", StringType(), True),\n",
    "        StructField(\"reviewUrl\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read the whole dataset as a batch\n",
    "kafkaStream = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\")\n",
    "    .option(\"subscribe\", \"review\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df = kafkaStream.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df1 = df.select(from_json(df.value, dataSchema.simpleString()))\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "sdf = df1.select(col(\"from_json(value).*\"))\n",
    "\n",
    "# Converting releaseDateTheaters to datetime\n",
    "review_df = sdf.withColumn(\n",
    "    \"creationDate\", to_date(col(\"creationDate\"), \"y-M-d\")\n",
    ")  # our dataframe with date column\n",
    "\n",
    "# Define the grade dictionary as a broadcast variable for efficient use in UDF\n",
    "grade_dct = spark.sparkContext.broadcast(\n",
    "    {\n",
    "        \"A+\": 0.985,\n",
    "        \"A\": 0.945,\n",
    "        \"A-\": 0.91,\n",
    "        \"B+\": 0.88,\n",
    "        \"B\": 0.845,\n",
    "        \"B-\": 0.81,\n",
    "        \"C+\": 0.78,\n",
    "        \"C\": 0.745,\n",
    "        \"C-\": 0.71,\n",
    "        \"D+\": 0.68,\n",
    "        \"D\": 0.645,\n",
    "        \"D-\": 0.61,\n",
    "        \"F\": 0.295,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Define the UDF for fixing the rating\n",
    "def fix_rating_udf(x):\n",
    "    try:\n",
    "        if x in grade_dct.value.keys():\n",
    "            return grade_dct.value[x]\n",
    "        else:\n",
    "            split_values = list(\n",
    "                map(\n",
    "                    lambda s: float(\n",
    "                        s.replace('\"', \"\").replace(\"'\", \"\").replace(\"*\", \"\").strip()\n",
    "                    ),\n",
    "                    x.split(\"/\"),\n",
    "                )\n",
    "            )\n",
    "            return split_values[0] / split_values[1]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Register the UDF\n",
    "fix_rating_spark_udf = udf(fix_rating_udf, FloatType())\n",
    "\n",
    "# Apply the UDF to the 'originalScore' column\n",
    "review_df = review_df.withColumn(\n",
    "    \"originalScore\", fix_rating_spark_udf(col(\"originalScore\"))\n",
    ")\n",
    "\n",
    "# Apply additional conditions to 'originalScore' column\n",
    "review_df = review_df.withColumn(\n",
    "    \"originalScore\", when(col(\"originalScore\") > 1, 1).otherwise(col(\"originalScore\"))\n",
    ")\n",
    "review_df = review_df.withColumn(\n",
    "    \"originalScore\", when(col(\"originalScore\") < 0, 0).otherwise(col(\"originalScore\"))\n",
    ")\n",
    "\n",
    "# Tokenize and lower the token\n",
    "word_df = review_df.withColumn(\"tokenized_review\", split(col(\"reviewText\"), \" \"))\n",
    "\n",
    "# Load the English stopwords set\n",
    "nltk.download(\"stopwords\")\n",
    "stop_wrds = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# Define a UDF to remove stopwords\n",
    "def remove_stopwords_udf(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stop_wrds]\n",
    "\n",
    "\n",
    "# Register the UDF\n",
    "remove_stopwords_spark_udf = udf(remove_stopwords_udf, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to the 'tokenized_review' column\n",
    "word_df = word_df.dropna(subset=\"tokenized_review\")\n",
    "word_df = word_df.withColumn(\n",
    "    \"filtered_review\", remove_stopwords_spark_udf(col(\"tokenized_review\"))\n",
    ")\n",
    "\n",
    "# Obtain the word counts per \"tokenized_review\"\n",
    "word_df = word_df.withColumn(\"word\", explode(\"filtered_review\"))\n",
    "# Group by the word and count the occurrences\n",
    "word_df = word_df.groupBy(\"id\", \"word\").count()\n",
    "\n",
    "\n",
    "# Get critic with harshes reviews\n",
    "\n",
    "\n",
    "avgratingf = review_df.groupBy(window(col(\"creationDate\"), \"1 day\")).agg(\n",
    "    avg(\"originalScore\")\n",
    ")\n",
    "\n",
    "rating_df = review_df.select(col(\"id\"), col(\"originalScore\"))\n",
    "\n",
    "query2 = (\n",
    "    word_df.orderBy(col(\"id\"))\n",
    "    .writeStream.queryName(\"token_count\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query1 = (\n",
    "    avgratingf.writeStream.queryName(\"avg_score\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query3 = (\n",
    "    rating_df.writeStream.queryName(\"rating\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "def my_foreach_batch_function_1(df, batch_id):\n",
    "    # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format(\"bigquery\").option(\n",
    "        \"table\", \"de23-398309.assignment2dataset.tokencount\"\n",
    "    ).mode(\"overwrite\").save()\n",
    "\n",
    "\n",
    "def my_foreach_batch_function_2(df, batch_id):\n",
    "    # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format(\"bigquery\").option(\n",
    "        \"table\", \"de23-398309.assignment2dataset.avgrating\"\n",
    "    ).mode(\"overwrite\").save()\n",
    "\n",
    "\n",
    "def my_foreach_batch_function_3(df, batch_id):\n",
    "    # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format(\"bigquery\").option(\n",
    "        \"table\", \"de23-398309.assignment2dataset.dist\"\n",
    "    ).mode(\"overwrite\").save()\n",
    "\n",
    "\n",
    "# Write to a sink - here, the output is written to a Big Query Table\n",
    "# Use your gcp bucket name.\n",
    "# ProcessingTime trigger with two-seconds micro-batch interval\n",
    "token_countQuery = (\n",
    "    word_df.orderBy(col(\"id\"))\n",
    "    .writeStream.outputMode(\"complete\")\n",
    "    .trigger(processingTime=\"2 seconds\")\n",
    "    .foreachBatch(my_foreach_batch_function_1)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "avg_ratingQuery = (\n",
    "    avgratingf.writeStream.outputMode(\"complete\")\n",
    "    .trigger(processingTime=\"2 seconds\")\n",
    "    .foreachBatch(my_foreach_batch_function_2)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "rating_distQuery = (\n",
    "    rating_df.writeStream.outputMode(\"append\")\n",
    "    .trigger(processingTime=\"2 seconds\")\n",
    "    .foreachBatch(my_foreach_batch_function_3)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "try:\n",
    "    token_countQuery.awaitTermination()\n",
    "    avg_ratingQuery.awaitTermination()\n",
    "    rating_distQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    token_countQuery.stop()\n",
    "    avg_ratingQuery.stop()\n",
    "    rating_distQuery.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "\n",
    "try:\n",
    "    for x in range(50):\n",
    "        spark.sql(\"SELECT * FROM token_count\").show()\n",
    "\n",
    "        spark.sql(\"SELECT * FROM avg_score\").show()\n",
    "\n",
    "        spark.sql(\"SELECT * FROM rating\").show()\n",
    "        sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "root\n",
    " |-- id: string (nullable = true)\n",
    " |-- reviewId: string (nullable = true)\n",
    " |-- creationDate: string (nullable = true)\n",
    " |-- criticName: string (nullable = true)\n",
    " |-- isTopCritic: string (nullable = true)\n",
    " |-- originalScore: string (nullable = true)\n",
    " |-- reviewState: string (nullable = true)\n",
    " |-- publicatioName: string (nullable = true)\n",
    " |-- reviewText: string (nullable = true)\n",
    " |-- scoreSentiment: string (nullable = true)\n",
    " |-- reviewUrl: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
